{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. sudo tar -C /usr -xzf ollama-linux-amd64.tgz\n",
    "2. ollama serve\n",
    "Open a new terminal\n",
    "#3. ollama run llama3.3\n",
    "#4. ollama run mistral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "import re\n",
    "from langchain.vectorstores import FAISS\n",
    "import pandas as pd\n",
    "from build_database import get_embedding_function\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "llama_model = Ollama(model=\"llama3.3\")\n",
    "s= llama_model('tell me a joke')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning Wrapper\n",
    "class ModelHyperparameterTuning:\n",
    "    def __init__(self, chunk_size, method):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.method = method\n",
    "\n",
    "    def load_vignettes(self, filepath, cat):\n",
    "        \"\"\"Load vignettes, categories, and ground truth labels from the CSV file.\"\"\"\n",
    "        data = pd.read_csv(filepath)\n",
    "        stress_vignettes = data#[data[\"Category\"] == cat]\n",
    "        vignettes = {}\n",
    "        for _, row in stress_vignettes.iterrows():\n",
    "            vignette_id = row['Vignette ID']\n",
    "            category = row['Category']\n",
    "            vignette = f\"{row['Referral']}\\n{row['Presenting Symptoms']}\\n{row['Additional Background Information']}\"\n",
    "            vignettes[vignette_id] = {\n",
    "                \"category\": category,\n",
    "                \"vignette_text\": vignette.strip(),\n",
    "                \"label\": row['Label']\n",
    "            }\n",
    "        return vignettes\n",
    "\n",
    "    def generate_diagnosis(self, rag_text, vignette_text, cot_text):\n",
    "        prompt = f\"\"\"\n",
    "        You are a professional clinician tasked with diagnosing based on the following vignette. \n",
    "\n",
    "\n",
    "        Clinical Guidelines:\n",
    "        {rag_text}\n",
    "\n",
    "        Vignette:\n",
    "        {vignette_text}\n",
    "        \n",
    "        Chain of Thought:\n",
    "        {cot_text}\n",
    "        \n",
    "        \n",
    "        Consider clinical guidelines and chian of thought carefully to reach a thoughtful diagnosis based on the vignette's details.\n",
    "\n",
    "        If you don't know the answer, just say that you don't know; don't try to make up an answer.\n",
    "        \"\"\"\n",
    "        response = llama_model.invoke(prompt)\n",
    "        return response\n",
    "\n",
    "    def rag(self, vignette):\n",
    "        embedding_function = get_embedding_function()\n",
    "        db = FAISS.load_local(\"Chroma_path\", embedding_function, allow_dangerous_deserialization=True)\n",
    "        results = db.similarity_search(vignette, k=self.chunk_size)\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in results])\n",
    "        return context_text\n",
    "\n",
    "    def rag_compress(self, vignette):\n",
    "        context_text = self.rag(vignette)\n",
    "        prompt = (\n",
    "            \"You are a professional summarizer and content organizer. Given the following extracted \"\n",
    "            \"information, compress and restructure it into a concise, well-organized summary while \"\n",
    "            \"preserving all critical details. Ensure the language is clear and the structure is logical.\\n\\n\"\n",
    "            \"Extracted Chunks:\\n\"\n",
    "            f\"{context_text}\\n\\n\"\n",
    "            \"Compressed and Restructured Summary:\"\n",
    "        )\n",
    "        compress_response = llama_model.invoke(prompt)\n",
    "        return compress_response\n",
    "\n",
    "    def cot(self): #cot_all\n",
    "        with open(\"cot_all.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            cot_text = file.read()\n",
    "        return cot_text\n",
    "\n",
    "    def cot_compress(self):\n",
    "        cot_text = self.cot()\n",
    "        prompt = (\n",
    "            \"You are a professional summarizer and content organizer. Given the following extracted \"\n",
    "            \"information, compress and restructure it into a concise, well-organized summary while \"\n",
    "            \"preserving all critical details. Ensure the language is clear and the structure is logical.\\n\\n\"\n",
    "            \"Extracted Chunks:\\n\"\n",
    "            f\"{cot_text}\\n\\n\"\n",
    "            \"Compressed and Restructured Summary:\"\n",
    "        )\n",
    "        compress_response = llama_model.invoke(prompt)\n",
    "        return compress_response\n",
    "\n",
    "    def get_rag_text(self, vignette_text):\n",
    "        if \"rag\" in self.method:\n",
    "            return self.rag_compress(vignette_text) if \"rag compress\" in self.method else self.rag(vignette_text)\n",
    "        return \"\"\n",
    "\n",
    "    def get_cot_text(self):\n",
    "        if \"cot\" in self.method:\n",
    "            return self.cot_compress() if \"cot compress\" in self.method else self.cot()\n",
    "        return \"\"\n",
    "\n",
    "    def extract_diagnosis(self, vignette_text):\n",
    "        \"\"\"Use the model to extract the diagnosis from the vignette text.\"\"\"\n",
    "        prompt = f\"Extract the final diagnosis from this text:\\n\\n{vignette_text}\"\n",
    "        response = llama_model(prompt)\n",
    "        return response.strip()\n",
    "\n",
    "    def compare_diagnosis(self, model_diagnosis, ground_truth_label):\n",
    "        \"\"\"Use the model to determine if the extracted diagnosis matches the ground truth.\"\"\"\n",
    "        prompt = (\n",
    "            f\"Given the diagnosis: '{model_diagnosis}'\\n\\n\"\n",
    "            f\"The ground truth diagnosis is: '{ground_truth_label}'.\\n\\n\"\n",
    "            f\"Does the final diagnosis match the ground truth? Give the similarity score (0-100) and provide reasoning.\"\n",
    "            f\"Example: 'Score: 85. The diagnoses are very similar because...\")\n",
    "        response = mistral_model(prompt).strip()\n",
    "        #prompt_score = f' {response},focus on the similairty score,give the similarity score, only numers without other string'\n",
    "        #score_extract = mistral_model(response)\n",
    "        #score_match = re.search(r\"(\\d+)\", score_extract)\n",
    "        #print(score_extract)\n",
    "\n",
    "        score_match = re.search(r\"(\\d+)\", response)\n",
    "\n",
    "        if score_match:\n",
    "            score = int(score_match.group(1))\n",
    "        else:\n",
    "            score = -1\n",
    "            #raise ValueError(\"Unable to parse similarity score from the response.\")\n",
    "        return score\n",
    "\n",
    "\n",
    "    def evaluate_model(self, vignettes):\n",
    "        \"\"\"Evaluate model performance by comparing predictions with ground truth labels.\"\"\"\n",
    "        correct_counts = {\"Mood\": 0, \"Anxiety\": 0, \"Stress\": 0}\n",
    "        total_counts = {\"Mood\": 0, \"Anxiety\": 0, \"Stress\": 0}\n",
    "        overall_correct = 0\n",
    "        total_count = len(vignettes)\n",
    "\n",
    "        cot_text = self.get_cot_text()\n",
    "\n",
    "        for vignette_id, vignette_data in vignettes.items():\n",
    "            category = vignette_data['category']\n",
    "            vignette_text = vignette_data['vignette_text']\n",
    "            ground_truth_label = vignette_data['label']\n",
    "\n",
    "            rag_text = self.get_rag_text(vignette_text)\n",
    "            model_diagnosis = self.generate_diagnosis(rag_text, vignette_text, cot_text).strip()\n",
    "            final_diag = self.extract_diagnosis(model_diagnosis)\n",
    "\n",
    "            score = self.compare_diagnosis(final_diag, ground_truth_label)\n",
    "            if score >= 70:\n",
    "                correct_counts[category] += 1\n",
    "                overall_correct += 1\n",
    "            total_counts[category] += 1\n",
    "\n",
    "            print(\"------------------------------------------------------------------------------\")\n",
    "            print(f\"Vignette ID: {vignette_id}\")\n",
    "            print(f\"Category: {category}\")\n",
    "            print(f\"Vignette Text:\\n{vignette_text}\\n\")\n",
    "            print(f\"Model Diagnosis: {model_diagnosis}\")\n",
    "            print(f\"The extracted Model Diagnosis: {final_diag}\")\n",
    "            print(f\"Ground Truth Label: {ground_truth_label}\")\n",
    "            print(\"The similarity score provided by the model:\", score)\n",
    "\n",
    "        for category in correct_counts:\n",
    "            category_accuracy = correct_counts[category] / total_counts[category] if total_counts[category] > 0 else 0\n",
    "            print(f\"Accuracy for {category}: {category_accuracy:.2f}\")\n",
    "\n",
    "        overall_accuracy = overall_correct / total_count if total_count > 0 else 0\n",
    "        print(f\"Overall Accuracy: {overall_accuracy:.2f}\")\n",
    "        \n",
    "        Counts = {\n",
    "            \"Mood\": correct_counts[\"Mood\"] ,\n",
    "            \"Anxiety\": correct_counts[\"Anxiety\"],\n",
    "            \"Stress\": correct_counts[\"Stress\"],\n",
    "            \"Overall\": overall_accuracy\n",
    "        }\n",
    "        Rates = {\"Mood\": correct_counts[\"Mood\"] / total_counts[\"Mood\"] if total_counts[\"Mood\"] > 0 else 0,\n",
    "            \"Anxiety\": correct_counts[\"Anxiety\"] / total_counts[\"Anxiety\"] if total_counts[\"Anxiety\"] > 0 else 0,\n",
    "            \"Stress\": correct_counts[\"Stress\"] / total_counts[\"Stress\"] if total_counts[\"Stress\"] > 0 else 0,\n",
    "            \"Overall\": overall_accuracy}\n",
    "        return Counts, Rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(filepath, category, chunk_size, method):\n",
    "    model = ModelHyperparameterTuning(chunk_size=chunk_size, method=method)\n",
    "    vignettes = model.load_vignettes(filepath, category)\n",
    "    #results = model.evaluate_model(vignettes)\n",
    "    counts, rates = model.evaluate_model(vignettes)\n",
    "    return rates\n",
    "    \n",
    "    \n",
    "all_accuracies = []\n",
    "for i in range(1):\n",
    "    llama_model = Ollama(model=\"llama3.3\")\n",
    "    mistral_model = Ollama(model ='mistral')\n",
    "    accuracy = run_model(\"Data_final.csv\", \"Stress\", chunk_size=10, method=\"rag \")\n",
    "    all_accuracies.append(accuracy)\n",
    "    print(f\"Run {i+1}: {accuracy}\")\n",
    "    gc.collect()\n",
    "    \n",
    "print(\"All Accuracies:\", all_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
